{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pandas.io.common import EmptyDataError\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Total number of words in the reddit data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the reddit dataset is 54987183\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/ssund/PycharmProjects/adm_project1/reddit\")\n",
    "\n",
    "word_count = 0\n",
    "for fileName in Path('.').glob('*.txt'):\n",
    "    with open(str(fileName.absolute()), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.split()\n",
    "            word_count += len(word)\n",
    "print(\"Total number of words in the reddit dataset is\", word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting WIKI words from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_word_dataset has been created\n"
     ]
    }
   ],
   "source": [
    "wiki1=\"C:/Users/ssund/PycharmProjects/adm_project1/wiki_word_dataset.txt\"\n",
    "f1= open(wiki1,\"w\")\n",
    "for fileName in Path('.').glob('*.txt'):  \n",
    "    lines = \"\"\n",
    "    wikiword = \"\"\n",
    "    with open(str(fileName.absolute()),'r',encoding=\"utf8\") as one_text:\n",
    "        lines=one_text.read()\n",
    "        splitwords=lines.split(\" \")    \n",
    "        for i in splitwords:\n",
    "            if (i.lower()).startswith(\"wiki_\"):       \n",
    "                wikiword=wikiword+i+\"\\n\"\n",
    "        f1.write(wikiword)\n",
    "f1.close()\n",
    "print(\"wiki_word_dataset has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the total number of Wiki Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of wiki words in the dataset: 1321967\n"
     ]
    }
   ],
   "source": [
    "wiki_data = \"C:/Users/ssund/PycharmProjects/adm_project1/wiki_word_dataset.txt\"\n",
    "count = len(open(wiki_data).readlines())\n",
    "print(\"Total number of wiki words in the dataset:\", count)\n",
    "\n",
    "os.chdir('C:/Users/ssund/PycharmProjects/adm_project1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the frequency of each Wiki word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"wiki_word_dataset.txt\", \"r\") as myfile:\n",
    "    data=myfile.read().replace('\\n', ' ')\n",
    "\n",
    "data = data.split(' ')\n",
    "fdist1 = nltk.FreqDist(data)\n",
    "sys.stdout = open(\"frequency.txt\",\"w\")\n",
    "print(fdist1.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the Wikifier add-ons at the beginning and at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open(\"wiki_word_dataset.txt\",'r')\n",
    "f2=open(\"wiki_clean.txt\",'w')\n",
    "\n",
    "completed_lines_hash = set()\n",
    "\n",
    "for w in f1:\n",
    "    f2.write(w.rpartition(\"__\")[0].replace(\"__\", \".\").replace(\"Wiki.\",\"\").replace(\".\",\" \").replace(\"_\",\" \").lower() + \"\\n\")\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of every word after removing the Wikifier add-ons at the beginning and at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"wiki_clean.txt\", \"r\") as myfile1:\n",
    "    data1=myfile1.read().replace('\\n', ' ')\n",
    "\n",
    "data1 = data1.split(' ')    \n",
    "fdist2 = nltk.FreqDist(data1)\n",
    "sys.stdout=open(\"wikiwords_frequency.txt\",\"w\")\n",
    "print(fdist2.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the duplicate words in the file before comparing with BERT Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"OUTPUT.txt\", \"w\")\n",
    "f=open(\"wiki_clean.txt\",'r')\n",
    "for line in f:\n",
    "  hashValue = hashlib.md5(line.rstrip().encode('utf-8')).hexdigest()\n",
    "  if hashValue not in completed_lines_hash:\n",
    "    output_file.write(line)\n",
    "    completed_lines_hash.add(hashValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the WikiWords with BERT vocabulary and printing them to a new text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = open('Matching_with_BERT.txt','w')\n",
    "file1 = set(line.strip() for line in open('OUTPUT.txt', 'r', encoding=\"utf8\"))\n",
    "file2 = set(line.strip() for line in open('BERT_Vocabulary.txt', 'r', encoding=\"utf8\"))\n",
    "\n",
    "for line in file1 & file2:\n",
    "\n",
    "    if line:\n",
    "        file3.write(line + \"\\n\")\n",
    "        \n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List 1: only the wiki words that contain a one single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open(\"OUTPUT.txt\",'r')\n",
    "b = open(\"single_words.txt\",'w')\n",
    "\n",
    "for i in a:\n",
    "    for j in i:\n",
    "        if j==\" \":\n",
    "            break\n",
    "    if j!=\" \":\n",
    "        b.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List 2: printing everything else into different lines because Bert expects only individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = open('OUTPUT.txt','r')\n",
    "c = open(\"multiple_words.txt\",'w')\n",
    "\n",
    "for i in d:\n",
    "    for j in i.split():\n",
    "        c.write(j)\n",
    "        c.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching both the lists to BERT Vocab and printing the words which are there in BERT Vocab to a new text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = set(line.strip() for line in open('single_words.txt', 'r', encoding=\"utf8\"))\n",
    "match_single = open('Final_Matching_Single_Words_with_BERT.txt','w')\n",
    "\n",
    "\n",
    "for line in e & file2:\n",
    "\n",
    "    if line:\n",
    "        match_single.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "f = set(line.strip() for line in open('multiple_words.txt', 'r', encoding=\"utf8\"))\n",
    "match_multiple = open('Final_Matching_Multiple_Words_with_BERT.txt','w')\n",
    "\n",
    "\n",
    "for line in f & file2:\n",
    "\n",
    "    if line:\n",
    "        match_multiple.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "file3.close()        \n",
    "sys.stdout.close()\n",
    "a.close()\n",
    "b.close()\n",
    "c.close()\n",
    "d.close()\n",
    "match_single.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the WIKI words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "f1 = open('Final_Matching_Multiple_Words_with_BERT.txt','r')\n",
    "f2 = open('analysis_multiple_word.txt', 'w')  \n",
    "doc = nlp(f1.read()) \n",
    "\n",
    "for ent in doc.ents: \n",
    "    op1 = ent.text + '\\n'\n",
    "    op2=ent.label_ + ' '\n",
    "    f2.write(op2)\n",
    "    f2.write(op1)\n",
    "    a=sorted(f2)  \n",
    "    for i in a:\n",
    "         s += i\n",
    "    f2.write(s)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread IPythonHistorySavingThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 780, in writeout_cache\n",
      "    self._writeout_input_cache(conn)\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 764, in _writeout_input_cache\n",
      "    (self.session_number,)+line)\n",
      "sqlite3.IntegrityError: UNIQUE constraint failed: history.session, history.line\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 834, in run\n",
      "    self.history_manager.writeout_cache(self.db)\n",
      "  File \"<C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-23>\", line 2, in writeout_cache\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 58, in needs_sqlite\n",
      "    return f(self, *a, **kw)\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 785, in writeout_cache\n",
      "    self.session_number)\n",
      "ValueError: I/O operation on closed file.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-24>\", line 2, in run\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 58, in needs_sqlite\n",
      "    return f(self, *a, **kw)\n",
      "  File \"C:\\Users\\ssund\\Anaconda3\\lib\\site-packages\\IPython\\core\\history.py\", line 837, in run\n",
      "    \"History will not be written to the database.\") % repr(e))\n",
      "ValueError: I/O operation on closed file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Analysis is done based on the below table from spacy library:\n",
    "\n",
    "# TYPE            DESCRIPTION\n",
    "# PERSON          People, including fictional.\n",
    "# NORP            Nationalities or religious or political groups.\n",
    "# FAC             Buildings, airports, highways, bridges, etc.\n",
    "# ORG             Companies, agencies, institutions, etc.\n",
    "# GPE             Countries, cities, states.\n",
    "# LOC             Non-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT         Objects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT           Named hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART     Titles of books, songs, etc.\n",
    "# LAW             Named documents made into laws.\n",
    "# LANGUAGE        Any named language.\n",
    "# DATE            Absolute or relative dates or periods.\n",
    "# TIME            Times smaller than a day.\n",
    "# PERCENT         Percentage, including ”%“.\n",
    "# MONEY           Monetary values, including unit.\n",
    "# QUANTITY        Measurements, as of weight or distance.\n",
    "# ORDINAL         “first”, “second”, etc.\n",
    "# CARDINAL        Numerals that do not fall under another type\n",
    "\n",
    "# SORT the above data:\n",
    "\n",
    "# f1=open('analysis_multiple_word.txt','r')\n",
    "# f2=open(\"analysis_multiple_word_sort.txt\",'w')\n",
    "# a=sorted(f1)\n",
    "# s=\"\"\n",
    "# for i in a:\n",
    "#     s += i\n",
    "# f2.write(s)\n",
    "# f1.close()\n",
    "# f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
